---
output:
  word_document:
    reference_docx: /Users/mdozmorov/Documents/Work/presentations/Templates/grant_template/styles.doc/NSF_grant_style.docx
  html_document: default
---

Reviewer reports:

> This manuscript introduces a method, "network coherence", and applies it to human disease genes (from GWAS); in addition to introducing a new method, they make a claim that the coherence measure varies among specific disorders in a biologically significant manner. As such this work needs to be evaluated in these two aspects: First, whether the method is novel and sufficiently well-validated. Second, whether the data support the claims about human phenotypes.

> Taking up the second aspect first, there is a major problem with using PPINs from BioGRID and STRING: these networks are extremely biased by "well-studied genes" or other types of artifacts. This means using them to make conclusions about anything biological is horribly confounded with the effects of human decisions about which proteins to use as targets among other issues. What the authors have failed to do is provide any evidence that their results are not simply due to such biases. Note that making random networks does not solve this problem - it is indeed "significant" that genes like TGFB have a lot of interactions, but it's far from clear that's due to biology. I believe all of the papers the authors cite in support of the idea that coherence (et al.) is a documented phenomenon associated with human diseases (pg 3 line 20) all have this issue, and the point of ref 27 has not, to my knowledge, been rebutted.

We agree that the PPI databases are potentially biased towards well-studied genes. Yet, databases like STRING try to avoid it by aggregating PPI information from multiple sources, including the predicted interactions. We intentionally used the full STRING database to maximally avoid PPI bias towards the well-studied genes. Yet, at this state of the knowledge, PPI databases remain incomplete (c.f. Menche et al. 2015). This was the rationale to mention the incompleteness of the databases throughout the manuscript. E.g., we discussed that our method "... will only improve as more PPI information becomes available", and devoted the whole paragraph explaining the limitations of PPI databases. 

While we acknowledge the incompleteness of the PPI databases and GWASs, the fundamental question is why some genes are studied much more than others? Historically, such genes emerge as a "low hanging fruit" in molecular biology experiments, consistently appear in GWAS studies, that is, representing the true biological signal. Consequently, they formed the backbone for forming the PPI databases, for building sets of GWAS loci. While we acknowledge the fact of bias, the source of it is likely biology-driven.

The bias and incompleteness of the PPI databases led us to use the permutation test. Both PPI databases and GWASs attempt to extract biological signal genome-wide. The fact that, e.g., TGFB got picked up by a GWAS study, and is highly represented in a PPI database is likely biology-driven. The fact that random networks have much less chance to pick up TGFB is due to the random, non-biological processes. The permutation test works in the same realm of PPIs that were used to build phenotype-specific networks. Being non-parametric, the permutation test equally treats genes with average degree distributions and those with extreme PPIs. If we had used parametric tests, chances of getting biased results would be high as outliers typically break the rules of parametric distributions leading to overinflated results.

> I have to question the limitation to GWAS loci. The authors concede that in effect their analysis cannot be used to conclude much of anything, due to gaps in knowledge about the genetics of the phenotypes (line 3 pg 14). Since the authors are aware of this problem, that they didn't use genes implicated across the spectrum of variant frequencies and effect sizes (including somatic variants in cancer) which are invisible to GWAS is surprising. So I wholeheartedly agree with their caveat, but the issue with the networks used is just as important. Overall, the results of the application of the method are very unconvincing to me.

We fully agree with the reviewer and intentionally chose our wording very carefully to highlight caveats in the current state of knowledge of both PPI databases and GWAS loci. Additionally, we carefully selected references highlighting the whole spectrum of current knowledge and views, e.g., intentionally including reference 27 as an alternative to the current state of knowledge supported by several other peer-reviewed references. Our goal was to present the method, the approach that can be used to compare molecular coherence of phenotypes while overcoming limitations of network analysis (differences in size, configuration). It was beyond the scope of our work to evaluate genes associated with rare variants, different effect sizes. The method itself can be applied to any GWAS results (see Discussion), including rare and somatic mutations, and reveal how coherent the associated gene networks are. We intentionally used the state-of-the-art list of GWAS results, and acknowledge the limitations of the current knowledge.

> None of this detracts, in principle, from the presentation of the method, since a method can exist and be novel in the absence of any useful or interesting application as these may emerge in the future. But as this is a bioinformatics paper, the standard format is to compare the results obtained on a gold standard, and to pit the method against other approaches either in reference to the gold standard (not that one could be defined here, except perhaps by simulations), or at least to the results obtained in actual use and describe the advantages, or at least differences, of the method to other approaches. The authors have done none of those things.

Indeed, the primary intention of our work is to present a principled method allowing for a comparison of the coherence of molecular mechanisms across phenotypes. While we are fully aware of the typical strategy for presenting novel bioinformatics methods (e.g., see https://www.biorxiv.org/content/10.1101/549170v3 for an example of such presentation), the problem here is that there are no other similar approaches that we are aware of. Nor is there a biological gold standard other than phenotype-specific gene networks used in our work. As described in the manuscript, random and KEGG networks are the best approximators of random and high coherence. The permutation framework is the most unbiased simulation strategy, in which we simulate random networks of the same size as phenotype-specific networks to get the significance of coherence estimates. 

> In terms of novelty, as the authors discuss, there are other measures of coherence, and one that is particularly relevant is modularity - various measures of "cluster tightness" relative to a network context go back many decades. Since modularity is literally the measure of the ratio of the density of in-group and to-group edges, and the authors repeatedly described modularity in just those terms (e.g. on pg 13), it isn't clear that method of the authors is even needed. 

We performed additional analyses of the association between modularity and coherence. Our results did not reveal a consistent association between coherence and modularity. We added the corresponding Results and Methods sections, discussed our observations, and updated the Additional Files 3 and 4 with modularity information. Again, our goal was to develop a general-purpose method for estimating and comparing coherence of molecular mechanisms across phenotypes, with the coherence normalization approach being one of the novel aspects. Normalization of modularity, while theoretically possible, would defy interpretability. Our results demonstrate that coherence is different from modularity. We hope that the simplicity, flexibility, and interpretability of our measure of coherence will be an important contribution to the field of network analysis in genomics.

> It is striking that the authors never provide an actual example of the node degree distributions for different phenotypes, since they form the basis for the regression analysis. As it stands, the novelty here seems minor, and directly comparing their method to other (including simpler) alternatives is essential.

We agree with the need to investigate and compare degree distributions. We added the results section and the new Figure 3 showing internal and external degree distributions for high- and low-coherence networks, and statistically compared them with size-matched random networks. Additional File 9 shows the results of the statistical comparison. In summary, the new results confirm the notion that networks of high coherence have a significantly larger number of internal and external connections than low-coherence networks and random networks.

Regarding the notion of comparison with other alternatives, please, see our response to the previous comment. 

> Another major drawback of the approach as presented is that there is no estimate of variance in the measure, that is, coherence should be treated as a random variable. In order to say that phenotype X has a higher coherence than phenotype Y, one would like to know the distributional properties of the point estimates, as I'm sure the authors are aware. It is not the same as saying a coherence value is different from random. The authors should provide confidence intervals etc. or remove all claims that one set of phenotypes has a "lower coherence" than another. The closest the authors get is comparing STRING and BioGRID (the authors do not mention their lack of independence; these databases overlap extensively). And the comparison is very qualitative anyway, for example saying on p9 line 22 that the "overall correlation remained high" without any numbers given.

